# The Pareto Rule in Machine Learning, Python Libraries

After being immersed in Data Science for several months, I already have a better understanding of the global environment, and the necessary skills as a Machine Learning engineer. It's just that sometimes I get imposter syndrome, and I start asking questions: Am I good enough? Can I really add value to the company I work for? Am I able to perform the tasks that are left to me?
Many more questions, and this is why I decided to do this project.

The Pareto Rule, also known as the 80/20 principle, is an empirical observation that states that approximately 80% of the effects come from 20% of the causes. This rule is applied in a variety of contexts and fields, and is used to highlight how a minority of items or factors often have a significantly greater impact than the majority.

I will use this rule in some of the fields as a ML engeener. The chosed language, with greater potential for growth and profit, is Python. The libraries where I will use this project, is going to be: Numpy, Pandas, Matplotlib and Seaborn, SQLite, Github, Terminal, scikit-learn, tensorflow, and Python as well. 

Wait, I had a better idea. Instead of asking him for specific libraries, I will prompt him to give me the top 20% of the causes that will give me the 80% of the results as a Data scientist :D!

## So, I asked to ChatGPT the next prompt: 
>Act as my professor in data science. If you had to condense in a Pareto diagram, everything there is to learn in data science, what would be the 20% of the causes that would give me 80% of the results? Mention languages, libraries, tools, and everything you think is relevant in this 20%

## ChatGPT Response:
```As your data science professor, I'd be happy to guide you through the essential 20% that can yield 80% of the results in data science. Focus on mastering these key aspects:

Programming Languages:

Python: The most widely used language in data science due to its versatility and extensive libraries.
Libraries:

NumPy: Fundamental package for numerical computing with Python.
Pandas: Used for data manipulation and analysis, particularly with structured data.
Matplotlib and Seaborn: Data visualization libraries to create graphs and plots.
Scikit-learn: Essential for machine learning algorithms and data preprocessing.
Data Manipulation:

SQL: For handling relational databases and querying data.
Pandas: For data wrangling, cleaning, and transformation.
Statistics and Mathematics:

Understanding of key statistical concepts like mean, median, standard deviation, and hypothesis testing.
Linear algebra and calculus basics.
Machine Learning:

Supervised Learning: Regression and Classification algorithms (e.g., Linear Regression, Decision Trees, Random Forest, Logistic Regression).
Unsupervised Learning: Clustering (e.g., K-Means, Hierarchical Clustering) and Dimensionality Reduction (e.g., Principal Component Analysis).
Evaluation metrics and cross-validation techniques.
Data Visualization:

Matplotlib and Seaborn for basic visualizations.
Understanding of data representation techniques.
Tools:

Jupyter Notebook/Lab: Interactive coding environment for data exploration and analysis.
Git: Version control system for collaborative projects.
GitHub/GitLab: Platforms for hosting and sharing code repositories.
Data Cleaning:

Handling missing data, outliers, and data inconsistencies.
Feature Engineering:

Creating meaningful features from raw data to improve model performance.
Problem Solving:

Structuring data science projects and defining problem statements.
Remember that this 20% represents the core foundation of data science, and mastering these concepts will give you a solid understanding of the field. However, data science is vast and continuously evolving, so dont hesitate to explore other areas that pique your interest once you have grasped these fundamentals. Practical hands-on projects and real-world datasets will further enhance your skills and expertise. Good luck with your data science journey!
```


## Pareto rule on libraries prompt:  
>Act as my Python tutor. You will use the Pareto rule. When I mention a specific library or something related, you will give me the fundamental principles to condense the most important 20% to know, in the 80% of the results of said library. The output must have the 20% of the causes in 10 numerated exercises. After writing the word "answers" you will give me the detailed solution of each exercise. Am I clear?

## His response:
```
 Absolutely clear! As your Python tutor using the Pareto rule, I will provide you with the fundamental principles of specific libraries or topics that will cover 20% of the most important concepts, leading to 80% of the results. I'll present these principles in 10 numerated exercises. Afterward, I'll provide the detailed solutions for each exercise once you prompt for "answers."
  
If you have any specific Python library or topic in mind, let me know, and we can start the exercise session!
```

And, finally, I'm just gonna input the specific library or tool, and will create a Notebook for that specific topic. If I can solve the exercise, without looking the response, I will mark it with a ":)", else: ":("

